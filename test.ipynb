{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and read the CSV file into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(i):\n",
    "    start = i*1000\n",
    "    path = \"data/mpd.slice.\" + str(start) + \"-\" + str(start+999) + \".json\"\n",
    "    data = json.load(open(path,'r'))\n",
    "    return pd.DataFrame.from_dict(data['playlists'], orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choose one of the following load options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OLD) Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tracks from playlists \n",
    "\n",
    "n = 2\n",
    "\n",
    "def get_train():\n",
    "    songPlaylistArray = []\n",
    "    start = 0\n",
    "    while start != n:\n",
    "         thisSlice = load(start)\n",
    "         songPlaylistArray.append(thisSlice)\n",
    "         start+= 1\n",
    "    return pd.concat(songPlaylistArray)\n",
    "\n",
    "# get table where each row represents a song within a playlist\n",
    "\n",
    "def get_train_playlist_songs():\n",
    "    exploded_train = train.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)\n",
    "\n",
    "#train = get_train()\n",
    "\n",
    "#train_playlist_songs = get_train_playlist_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# combination of get_train and get_train_playlist_songs\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = this_slice\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, this_slice])\n",
    "        start += 1\n",
    "    \n",
    "    exploded_train = concatenated_data.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    \n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined()\n",
    "#train_playlist_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# contains only pid and track_uri\n",
    "\n",
    "import time\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined_small():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        this_slice = this_slice[['pid', 'tracks']]\n",
    "\n",
    "        exploded_slice = this_slice.explode('tracks')\n",
    "        exploded_slice.reset_index(drop=True, inplace=True)\n",
    "        normalized_tracks = pd.json_normalize(exploded_slice['tracks'])\n",
    "        normalized_tracks = normalized_tracks[['track_uri']]\n",
    "\n",
    "        playlist_songs = pd.concat([exploded_slice.drop(columns='tracks'), normalized_tracks], axis=1)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = playlist_songs\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, playlist_songs])\n",
    "        start += 1\n",
    "\n",
    "        if(start % 1 == 0):\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"playlist {start*1000} loaded in {execution_time:.2f} seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(\"all playlists loaded\")\n",
    "\n",
    "    concatenated_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined_small()\n",
    "#len(train_playlist_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to table where each row represents a song within a playlist, and which contains only playlist name, pid and track_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# contains only pid and track_uri\n",
    "\n",
    "import time\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined_small_2():\n",
    "    start = 0\n",
    "    result_tracks = None\n",
    "    result_names = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while start != n:\n",
    "        slice = load(start)\n",
    "\n",
    "        slice_tracks = slice[['pid', 'tracks']]\n",
    "        slice_names = slice[['pid', 'name']]\n",
    "\n",
    "        exploded_slice = slice_tracks.explode('tracks')\n",
    "        exploded_slice.reset_index(drop=True, inplace=True)\n",
    "        normalized_tracks = pd.json_normalize(exploded_slice['tracks'])\n",
    "        normalized_tracks = normalized_tracks[['track_uri']]\n",
    "\n",
    "        playlist_songs = pd.concat([exploded_slice.drop(columns='tracks'), normalized_tracks], axis=1)\n",
    "\n",
    "        if result_tracks is None:\n",
    "            result_tracks = playlist_songs\n",
    "            result_names = slice_names\n",
    "        else:\n",
    "            result_tracks = pd.concat([result_tracks, playlist_songs])\n",
    "            result_names = pd.concat([result_names, slice_names])\n",
    "        start += 1\n",
    "\n",
    "        if(start % 1 == 0):\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"playlist {start*1000} loaded in {execution_time:.2f} seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(\"all playlists loaded\")\n",
    "\n",
    "    result_tracks.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result_tracks, result_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs, train_names = get_train_playlist_songs_combined_small_2()\n",
    "#train_playlist_songs, train_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore data from pickle file\n",
    "\n",
    "#train_playlist_songs = pd.read_pickle(\"my_data.pkl\")\n",
    "#len(train_playlist_songs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "\n",
    "#train_playlist_songs.to_pickle(\"my_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Divide dataset to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and evaluate\n",
    "\n",
    "unique_playlists = train_playlist_songs['pid'].unique()\n",
    "\n",
    "train_playlists, evaluate_playlists = train_test_split(unique_playlists, test_size=0.2, random_state=42)\n",
    "\n",
    "train = train_playlist_songs[train_playlist_songs['pid'].isin(train_playlists)]\n",
    "evaluate = train_playlist_songs[train_playlist_songs['pid'].isin(evaluate_playlists)]\n",
    "\n",
    "#train_playlist_songs = train\n",
    "#evaluate_playlists_songs = evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Methode 1: Cold start recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a song with number of times it appears in playlists\n",
    "\n",
    "count_df = train_playlist_songs['track_uri'].value_counts().reset_index()\n",
    "\n",
    "count_df.columns = ['track_uri', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 most popular songs \n",
    "\n",
    "def get_first_tracks(n):\n",
    "    return count_df[:n]['track_uri'].tolist()\n",
    "\n",
    "#first_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Methode 2: User-based recommendation (based on playlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range by most frequent songs\n",
    "def get_most_freq_songs():\n",
    "    most_freq_songs = pd.DataFrame(train_playlist_songs.groupby(['track_uri'])['pid'].count()).reset_index().rename(columns= {'pid':'song_freq'}).sort_values(by=['song_freq'], ascending=False)\n",
    "    return most_freq_songs\n",
    "\n",
    "most_freq_songs = get_most_freq_songs()\n",
    "#most_freq_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI's of user's prefered tracks as list\n",
    "\n",
    "def get_pref_tracksURI_list(df_user_PL):\n",
    "    pref_tracksURI = [el['track_uri'] for el in df_user_PL['tracks']]\n",
    "\n",
    "    # Remove doublons\n",
    "    pref_tracksURI_list = list(set(pref_tracksURI))\n",
    "    return pref_tracksURI_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select playlists contained user's prefered tracks\n",
    "def get_most_relevant_playlist(pref_tracksURI_list):\n",
    "    pref_in_train = train_playlist_songs.loc[train_playlist_songs['track_uri'].isin(pref_tracksURI_list)]\n",
    "    \n",
    "    # Range them according to relevance\n",
    "    most_relevant = pref_in_train['pid'].value_counts().rename_axis('pid').reset_index(name='Frequency')\n",
    "    return most_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for a playlist\n",
    "def get_recommendation_for_playlist(playlist, n):\n",
    "\n",
    "    # if initial playlist is empty, return 500 most frequent tracks\n",
    "    if len(playlist) == 0:\n",
    "        return get_first_tracks(n)\n",
    "    \n",
    "    most_relevant = get_most_relevant_playlist(playlist)\n",
    "\n",
    "    pids = most_relevant['pid'].tolist()\n",
    "\n",
    "    # Exclude preferencies from proposition\n",
    "    sans_preferences = train_playlist_songs[train_playlist_songs.pid.isin(pids) & ~train_playlist_songs.track_uri.isin(playlist)][['pid', 'track_uri']]\n",
    "\n",
    "    # Add playlist frequency info\n",
    "    new_one = sans_preferences.merge(most_relevant, left_on='pid', right_on='pid').rename(columns= {'Frequency':'playlist_freq'})\n",
    "\n",
    "    # Add track frequency info, sort by playlist_freq first then by song_freq\n",
    "    new_one_1 = new_one.merge(most_freq_songs, left_on='track_uri', right_on='track_uri').sort_values(by=['playlist_freq', 'song_freq'], ascending=False)\n",
    "\n",
    "    # Exclude doublons from proposition\n",
    "    sans_doublons = new_one_1[['track_uri', 'playlist_freq', 'song_freq']].drop_duplicates(['track_uri'])\n",
    "\n",
    "    # First 500 tracks\n",
    "    recommended = sans_doublons[:n]['track_uri'].tolist()\n",
    "    \n",
    "    if len(recommended) < n:\n",
    "        songs_not_in_list = count_df[~count_df['track_uri'].isin(recommended)]['track_uri']\n",
    "        songs_to_add = songs_not_in_list.head(n - len(recommended)).tolist()\n",
    "        recommended.extend(songs_to_add)\n",
    "            \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Methode 3: Tracklist title analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('word2vec_sample')\n",
    "nltk.download('all')\n",
    "#nltk.download('punkt')            #if arror need to download\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little cleaning\n",
    "def prepare_titles(df_train):\n",
    "    liste = (df_train[['name']]\n",
    "          .drop_duplicates(['name'])['name']\n",
    "          .str.lower()\n",
    "          .str.lstrip(string.punctuation)\n",
    "          .str.lstrip(string.whitespace)\n",
    "          .str.rstrip(string.punctuation)\n",
    "          .str.rstrip(string.whitespace)\n",
    "          .tolist()\n",
    "        )   \n",
    "    #remove doblicates again\n",
    "    return list(set(liste))\n",
    "\n",
    "\n",
    "# Function to calculate similarity score between two words\n",
    "def get_synonym_score(word1, word2):\n",
    "    synsets_word1 = wordnet.synsets(word1)\n",
    "    synsets_word2 = wordnet.synsets(word2)\n",
    "\n",
    "    if synsets_word1 and synsets_word2:\n",
    "        syn_score = synsets_word1[0].path_similarity(synsets_word2[0])\n",
    "        return syn_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Calculate similarity between words from both sentences\n",
    "def get_similarity_between_words(words_sentence1, words_sentence2):\n",
    "    words_sentence1 = words_sentence1.split()\n",
    "    words_sentence2 = words_sentence2.split()\n",
    "    symres = 0\n",
    "    for word1 in words_sentence1:\n",
    "        for word2 in words_sentence2:\n",
    "            similarity_score = get_synonym_score(word1, word2)\n",
    "            if similarity_score is not None:\n",
    "                symres += similarity_score\n",
    "    maxlen = max(len(words_sentence1), len(words_sentence2))\n",
    "    return symres/maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "list_titles = prepare_titles(train_names)\n",
    "\n",
    "encoded_titles_pairs = [(item, model.encode(item)) for item in list_titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find <k> the most similare titles of playlists for <title>\n",
    "def find_similar_titles(title, k):    \n",
    "    simularity = {}\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    sample_embed = model.encode(title, convert_to_tensor=True)\n",
    "    \n",
    "    for el,curr_embed in encoded_titles_pairs:\n",
    "            \n",
    "        #Compute cosine-similarities\n",
    "        cosine_scores = util.cos_sim(sample_embed, curr_embed)\n",
    "        \n",
    "        if (cosine_scores.item() >0.5):\n",
    "            simularity[el] = cosine_scores.item()\n",
    "\n",
    "    # sort by values\n",
    "    ret = dict(sorted(simularity.items(), key=lambda x:x[1], reverse = True))\n",
    "    \n",
    "    if (k < len(simularity)):\n",
    "        print(f\"Get first {k} simular titles from {len(ret)} with similarity score > 0.5:\")        \n",
    "        return dict(list(ret.items())[0: k])\n",
    "    else:\n",
    "        print(f\"{len(ret)} Simular titles from with similarity score > 0.5:\") \n",
    "        return ret\n",
    "\n",
    "\n",
    "# Find in titles <=k synonymes of title\n",
    "def get_similarity_for_title(title, titles, k):\n",
    "    simularity = {}\n",
    "\n",
    "    for word in titles:\n",
    "        similarity_score = get_similarity_between_words(title, word)\n",
    "        if similarity_score is not None:\n",
    "            if similarity_score > 0.5 :\n",
    "                simularity[word] = similarity_score\n",
    "                    \n",
    "    ret = dict(sorted(simularity.items(), key=lambda x:x[1], reverse = True))\n",
    "    \n",
    "    if (k < len(ret)):\n",
    "        print(f\"Get first {k} synonyme titles from {len(ret)} with similarity score > 0.5:\")        \n",
    "        return dict(list(ret.items())[0: k])\n",
    "    else:\n",
    "        print(f\"{len(ret)} synonyme titles from with similarity score > 0.5:\") \n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_by_title(pid, k):\n",
    "\n",
    "    title = train_names.loc[train_names['pid'] == pid]['name'].iloc[0]\n",
    "\n",
    "    return find_similar_titles(title, k)\n",
    "\n",
    "get_recommendation_by_title(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref1 = ['dance']\n",
    "pref2 = ['uplift']\n",
    "pref3 = ['Groovin']\n",
    "\n",
    "list_titles = prepare_titles(train_names)\n",
    "\n",
    "rez = find_similar_titles(pref1[0], list_titles, 10)\n",
    "print(rez)\n",
    "print()\n",
    "\n",
    "similarity_list = get_similarity_for_title(pref1[0], list_titles, 10)\n",
    "print(similarity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Efficiency calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dcg(recommended, evaluations):\n",
    "    rel = []\n",
    "\n",
    "    for track in recommended:\n",
    "        if track in evaluations:\n",
    "            rel.append(evaluations.index(track))\n",
    "        else:\n",
    "            rel.append(0)\n",
    "\n",
    "    dcg = rel[0]\n",
    "    dcgi = 1\n",
    "    for i in range(1, len(rel)):\n",
    "        dcg += rel[i] / math.log2(i + 1)\n",
    "        dcgi += 1 / math.log2(i + 1)  \n",
    "    return dcg/dcgi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using User-based recommendation with challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load challenge set\n",
    "\n",
    "path = \"data_challenge/challenge_set.json\"\n",
    "data = json.load(open(path,'r'))\n",
    "data_challenge = pd.DataFrame.from_dict(data['playlists'], orient='columns')\n",
    "\n",
    "challenge_set = [data_challenge]\n",
    "df_challenge = pd.concat(challenge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for each playlist in challenge set, write to file\n",
    "\n",
    "import os\n",
    "\n",
    "file_name = 'result.csv'\n",
    "file_number = 1\n",
    "\n",
    "while os.path.exists(f\"{file_name[:-4]}_{file_number}.csv\"):\n",
    "    file_number += 1\n",
    "\n",
    "new_file_name = f\"{file_name[:-4]}_{file_number}.csv\"\n",
    "\n",
    "\n",
    "with open(new_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    if file.tell() == 0:\n",
    "        file.write('team_info,Bragina_Graff,vdfrtrp@gmail.com\\n\\n')\n",
    "\n",
    "    for i in range(1004, 1005):\n",
    "        playlist = df_challenge.iloc[i]\n",
    "\n",
    "        recommended = get_recommendation_for_playlist(playlist, 500)\n",
    "        \n",
    "        pid = playlist['pid']\n",
    "        \n",
    "        recommended_str = ', '.join(recommended)\n",
    "        file.write(f\"{pid}, {recommended_str}\\n\\n\")\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(f\"playlist {i} was processed\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_algorithm(row, algorithm):\n",
    "    pid = row['pid']\n",
    "\n",
    "    tracks = row['tracks']\n",
    "\n",
    "    tests, evaluations = train_test_split(tracks, test_size=0.2, random_state=42)\n",
    "\n",
    "    recommended = algorithm(tests, 500)\n",
    "\n",
    "    intersection = [item for item in recommended if item in evaluations]\n",
    "\n",
    "    precision = len(intersection) / len(recommended)\n",
    "    recall = len(intersection) / len(evaluations)\n",
    "    dcgs = calculate_dcg(recommended, evaluations)\n",
    "\n",
    "    return precision, recall, dcgs\n",
    "\n",
    "def evaluate_algorithm(algorithm, tracks):\n",
    "    \n",
    "    results = tracks.apply(lambda row: apply_algorithm(row, algorithm), axis=1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results.tolist(), columns=['precision', 'recall', 'dcgs'])\n",
    "    \n",
    "    average_precision = results_df['precision'].mean()\n",
    "    average_recall = results_df['recall'].mean()\n",
    "    average_dcgs = results_df['dcgs'].mean()\n",
    "    \n",
    "    print(f\"Average precision: {average_precision}\")\n",
    "    print(f\"Average recall: {average_recall}\")\n",
    "    print(f\"Average dcg: {average_dcgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of list of tracks of each playlist\n",
    "\n",
    "playlist_tracks = evaluate_playlists_songs.groupby('pid')['track_uri'].apply(list).reset_index(name='tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = playlist_tracks[:2]\n",
    "print(len(tracks))\n",
    "\n",
    "algorithm = lambda playlist, n: get_recommendation_for_playlist(playlist, n)\n",
    "evaluate_algorithm(algorithm, tracks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
