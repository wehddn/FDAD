{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and read the CSV file into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(i):\n",
    "    start = i*1000\n",
    "    path = \"data/mpd.slice.\" + str(start) + \"-\" + str(start+999) + \".json\"\n",
    "    data = json.load(open(path,'r'))\n",
    "    return pd.DataFrame.from_dict(data['playlists'], orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Choose one of the following load options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OLD) Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tracks from playlists \n",
    "\n",
    "n = 2\n",
    "\n",
    "def get_train():\n",
    "    songPlaylistArray = []\n",
    "    start = 0\n",
    "    while start != n:\n",
    "         thisSlice = load(start)\n",
    "         songPlaylistArray.append(thisSlice)\n",
    "         start+= 1\n",
    "    return pd.concat(songPlaylistArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = get_train()\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a song within a playlist\n",
    "\n",
    "def get_train_playlist_songs():\n",
    "    exploded_train = train.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# combination of get_train and get_train_playlist_songs\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = this_slice\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, this_slice])\n",
    "        start += 1\n",
    "    \n",
    "    exploded_train = concatenated_data.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    \n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined()\n",
    "#train_playlist_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# contains only pid and track_uri\n",
    "\n",
    "import time\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined_small():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        this_slice = this_slice[['pid', 'tracks']]\n",
    "\n",
    "        exploded_slice = this_slice.explode('tracks')\n",
    "        exploded_slice.reset_index(drop=True, inplace=True)\n",
    "        normalized_tracks = pd.json_normalize(exploded_slice['tracks'])\n",
    "        normalized_tracks = normalized_tracks[['track_uri']]\n",
    "\n",
    "        playlist_songs = pd.concat([exploded_slice.drop(columns='tracks'), normalized_tracks], axis=1)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = playlist_songs\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, playlist_songs])\n",
    "        start += 1\n",
    "\n",
    "        if(start % 1 == 0):\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"playlist {start*1000} loaded in {execution_time:.2f} seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(\"all playlists loaded\")\n",
    "\n",
    "    concatenated_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined_small()\n",
    "#len(train_playlist_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore data from pickle file\n",
    "\n",
    "#train_playlist_songs = pd.read_pickle(\"my_data.pkl\")\n",
    "#len(train_playlist_songs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "\n",
    "#train_playlist_songs.to_pickle(\"my_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User-based recommendation (based on playlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI's of user's prefered tracks as list\n",
    "\n",
    "def get_pref_tracksURI_list(df_user_PL):\n",
    "    pref_tracksURI = [el['track_uri'] for el in df_user_PL['tracks']]\n",
    "\n",
    "    # Remove doublons\n",
    "    pref_tracksURI_list = list(set(pref_tracksURI))\n",
    "    return pref_tracksURI_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a song with number of times it appears in playlists\n",
    "\n",
    "count_df = train_playlist_songs['track_uri'].value_counts().reset_index()\n",
    "\n",
    "count_df.columns = ['track_uri', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 most popular songs \n",
    "\n",
    "def get_first_tracks(n):\n",
    "    return count_df[:n]['track_uri'].tolist()\n",
    "\n",
    "#first_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select playlists contained user's prefered tracks\n",
    "def get_most_relevant(pref_tracksURI_list):\n",
    "    pref_in_train = train_playlist_songs.loc[train_playlist_songs['track_uri'].isin(pref_tracksURI_list)]\n",
    "    pref_in_train\n",
    "\n",
    "    # Range them according to relevance\n",
    "    most_relevant = pref_in_train['pid'].value_counts().rename_axis('pid').reset_index(name='Frequency')\n",
    "    return most_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range by most frequent songs\n",
    "def get_most_freq_songs():\n",
    "    most_freq_songs = pd.DataFrame(train_playlist_songs.groupby(['track_uri'])['pid'].count()).reset_index().rename(columns= {'pid':'song_freq'}).sort_values(by=['song_freq'], ascending=False)\n",
    "    return most_freq_songs\n",
    "\n",
    "most_freq_songs = get_most_freq_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User-based recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of playlists in range of relevance\n",
    "def get_recommendation(playlist, n):\n",
    "    pref_tracksURI_list = get_pref_tracksURI_list(playlist)\n",
    "    \n",
    "    # if initial playlist is empty, return 500 most frequent tracks\n",
    "    if len(pref_tracksURI_list) == 0:\n",
    "        return get_first_tracks(n)\n",
    "    \n",
    "    most_relevant = get_most_relevant(pref_tracksURI_list)\n",
    "\n",
    "    pids = most_relevant['pid'].tolist()\n",
    "\n",
    "    # Exclude preferencies from proposition\n",
    "    sans_preferences = train_playlist_songs[train_playlist_songs.pid.isin(pids) & ~train_playlist_songs.track_uri.isin(pref_tracksURI_list)][['pid', 'track_uri']]\n",
    "\n",
    "    # Add playlist frequency info\n",
    "    new_one = sans_preferences.merge(most_relevant, left_on='pid', right_on='pid').rename(columns= {'Frequency':'playlist_freq'})\n",
    "\n",
    "    # Add track frequency info, sort by playlist_freq first then by song_freq\n",
    "    new_one_1 = new_one.merge(most_freq_songs, left_on='track_uri', right_on='track_uri').sort_values(by=['playlist_freq', 'song_freq'], ascending=False)\n",
    "\n",
    "    # Exclude doublons from proposition\n",
    "    sans_doublons = new_one_1[['track_uri', 'playlist_freq', 'song_freq']].drop_duplicates(['track_uri'])\n",
    "\n",
    "    # First 500 tracks\n",
    "    recommended = sans_doublons[:n]['track_uri'].tolist()\n",
    "    \n",
    "    if len(recommended) < n:\n",
    "        songs_not_in_list = count_df[~count_df['track_uri'].isin(recommended)]['track_uri']\n",
    "        songs_to_add = songs_not_in_list.head(n - len(recommended)).tolist()\n",
    "        recommended.extend(songs_to_add)\n",
    "            \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommended = get_recommendation()\n",
    "#recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using User-based recommendation with challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load challenge set\n",
    "\n",
    "path = \"data_challenge/challenge_set.json\"\n",
    "data = json.load(open(path,'r'))\n",
    "data_challenge = pd.DataFrame.from_dict(data['playlists'], orient='columns')\n",
    "\n",
    "challenge_set = [data_challenge]\n",
    "df_challenge = pd.concat(challenge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for each playlist in challenge set, write to file\n",
    "\n",
    "import os\n",
    "\n",
    "file_name = 'result.csv'\n",
    "file_number = 1\n",
    "\n",
    "while os.path.exists(f\"{file_name[:-4]}_{file_number}.csv\"):\n",
    "    file_number += 1\n",
    "\n",
    "new_file_name = f\"{file_name[:-4]}_{file_number}.csv\"\n",
    "\n",
    "\n",
    "with open(new_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    if file.tell() == 0:\n",
    "        file.write('team_info,Bragina_Graff,vdfrtrp@gmail.com\\n\\n')\n",
    "\n",
    "    for i in range(1004, 1005):\n",
    "        playlist = df_challenge.iloc[i]\n",
    "\n",
    "        recommended = get_recommendation(playlist)\n",
    "        \n",
    "        pid = playlist['pid']\n",
    "        \n",
    "        recommended_str = ', '.join(recommended)\n",
    "        file.write(f\"{pid}, {recommended_str}\\n\\n\")\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(f\"playlist {i} was processed\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using User-based recommendation with 1 track "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name                                                          Funk\n",
      "collaborative                                                false\n",
      "pid                                                          10000\n",
      "modified_at                                             1470355200\n",
      "num_tracks                                                      16\n",
      "num_albums                                                      15\n",
      "num_followers                                                    1\n",
      "tracks           [{'pos': 0, 'artist_name': 'Coldplay', 'track_...\n",
      "num_edits                                                        4\n",
      "Name: 0, dtype: object, duration_ms    4007017\n",
      "num_artists         11\n",
      "description        NaN\n",
      "Name: 0, dtype: object]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_algorithm(algorithm, playlists):\n",
    "    # Divide each playlist into 2 parts: first 80% and last 20%\n",
    "    parts = []\n",
    "    for playlist in playlists:\n",
    "        parts.append(playlist[:int(len(playlist)*0.8)])\n",
    "        parts.append(playlist[int(len(playlist)*0.8):])\n",
    "\n",
    "    print(parts)\n",
    "\n",
    "    # Make recommendation for first 80% and compare to last 20%\n",
    "    # Calculate precision and recall\n",
    "    # Calculate average precision and recall for all playlists\n",
    "    # Calculate F1 score\n",
    "    # Return average F1 score\n",
    "\n",
    "playlists = [load(10).iloc[0]]\n",
    "\n",
    "algorithm = lambda playlist, n: get_recommendation(playlist, n)\n",
    "evaluate_algorithm(algorithm, playlists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
