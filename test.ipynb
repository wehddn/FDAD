{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and read the CSV file into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(i):\n",
    "    start = i*1000\n",
    "    path = \"data/mpd.slice.\" + str(start) + \"-\" + str(start+999) + \".json\"\n",
    "    data = json.load(open(path,'r'))\n",
    "    return pd.DataFrame.from_dict(data['playlists'], orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Choose one of the following load options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OLD) Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tracks from playlists \n",
    "\n",
    "n = 2\n",
    "\n",
    "def get_train():\n",
    "    songPlaylistArray = []\n",
    "    start = 0\n",
    "    while start != n:\n",
    "         thisSlice = load(start)\n",
    "         songPlaylistArray.append(thisSlice)\n",
    "         start+= 1\n",
    "    return pd.concat(songPlaylistArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = get_train()\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a song within a playlist\n",
    "\n",
    "def get_train_playlist_songs():\n",
    "    exploded_train = train.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# combination of get_train and get_train_playlist_songs\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = this_slice\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, this_slice])\n",
    "        start += 1\n",
    "    \n",
    "    exploded_train = concatenated_data.explode('tracks')\n",
    "    exploded_train.reset_index(drop=True, inplace=True)\n",
    "    normalized_tracks = pd.json_normalize(exploded_train['tracks'])\n",
    "    \n",
    "    return pd.concat([exploded_train.drop(columns='tracks'), normalized_tracks], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined()\n",
    "#train_playlist_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a playlist with 1 song\n",
    "# contains only pid and track_uri\n",
    "\n",
    "import time\n",
    "\n",
    "n = 1\n",
    "\n",
    "def get_train_playlist_songs_combined_small():\n",
    "    start = 0\n",
    "    concatenated_data = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while start != n:\n",
    "        this_slice = load(start)\n",
    "\n",
    "        this_slice = this_slice[['pid', 'tracks']]\n",
    "\n",
    "        exploded_slice = this_slice.explode('tracks')\n",
    "        exploded_slice.reset_index(drop=True, inplace=True)\n",
    "        normalized_tracks = pd.json_normalize(exploded_slice['tracks'])\n",
    "        normalized_tracks = normalized_tracks[['track_uri']]\n",
    "\n",
    "        playlist_songs = pd.concat([exploded_slice.drop(columns='tracks'), normalized_tracks], axis=1)\n",
    "\n",
    "        if concatenated_data is None:\n",
    "            concatenated_data = playlist_songs\n",
    "        else:\n",
    "            concatenated_data = pd.concat([concatenated_data, playlist_songs])\n",
    "        start += 1\n",
    "\n",
    "        if(start % 1 == 0):\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"playlist {start*1000} loaded in {execution_time:.2f} seconds\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(\"all playlists loaded\")\n",
    "\n",
    "    concatenated_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_playlist_songs = get_train_playlist_songs_combined_small()\n",
    "#len(train_playlist_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to table where each row represents a song within a playlist, and which contains only pid and track_uri from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore data from pickle file\n",
    "\n",
    "#train_playlist_songs = pd.read_pickle(\"my_data.pkl\")\n",
    "#len(train_playlist_songs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "\n",
    "#train_playlist_songs.to_pickle(\"my_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User-based recommendation (based on playlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_playlists = train_playlist_songs['pid'].unique()\n",
    "\n",
    "train_playlists, evaluate_playlists = train_test_split(unique_playlists, test_size=0.2, random_state=42)\n",
    "\n",
    "train = train_playlist_songs[train_playlist_songs['pid'].isin(train_playlists)]\n",
    "evaluate = train_playlist_songs[train_playlist_songs['pid'].isin(evaluate_playlists)]\n",
    "\n",
    "#train_playlist_songs = train\n",
    "#evaluate_playlists_songs = evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI's of user's prefered tracks as list\n",
    "\n",
    "def get_pref_tracksURI_list(df_user_PL):\n",
    "    pref_tracksURI = [el['track_uri'] for el in df_user_PL['tracks']]\n",
    "\n",
    "    # Remove doublons\n",
    "    pref_tracksURI_list = list(set(pref_tracksURI))\n",
    "    return pref_tracksURI_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table where each row represents a song with number of times it appears in playlists\n",
    "\n",
    "count_df = train_playlist_songs['track_uri'].value_counts().reset_index()\n",
    "\n",
    "count_df.columns = ['track_uri', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 most popular songs \n",
    "\n",
    "def get_first_tracks(n):\n",
    "    return count_df[:n]['track_uri'].tolist()\n",
    "\n",
    "#first_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select playlists contained user's prefered tracks\n",
    "def get_most_relevant(pref_tracksURI_list):\n",
    "    pref_in_train = train_playlist_songs.loc[train_playlist_songs['track_uri'].isin(pref_tracksURI_list)]\n",
    "    pref_in_train\n",
    "\n",
    "    # Range them according to relevance\n",
    "    most_relevant = pref_in_train['pid'].value_counts().rename_axis('pid').reset_index(name='Frequency')\n",
    "    return most_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range by most frequent songs\n",
    "def get_most_freq_songs():\n",
    "    most_freq_songs = pd.DataFrame(train_playlist_songs.groupby(['track_uri'])['pid'].count()).reset_index().rename(columns= {'pid':'song_freq'}).sort_values(by=['song_freq'], ascending=False)\n",
    "    return most_freq_songs\n",
    "\n",
    "most_freq_songs = get_most_freq_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User-based recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for a playlist\n",
    "def get_recommendation_for_playlist(playlist, n):\n",
    "    pref_tracksURI_list = get_pref_tracksURI_list(playlist)\n",
    "\n",
    "    # if initial playlist is empty, return 500 most frequent tracks\n",
    "    if len(pref_tracksURI_list) == 0:\n",
    "        return get_first_tracks(n)\n",
    "    \n",
    "    most_relevant = get_most_relevant(pref_tracksURI_list)\n",
    "\n",
    "    pids = most_relevant['pid'].tolist()\n",
    "\n",
    "    # Exclude preferencies from proposition\n",
    "    sans_preferences = train_playlist_songs[train_playlist_songs.pid.isin(pids) & ~train_playlist_songs.track_uri.isin(pref_tracksURI_list)][['pid', 'track_uri']]\n",
    "\n",
    "    # Add playlist frequency info\n",
    "    new_one = sans_preferences.merge(most_relevant, left_on='pid', right_on='pid').rename(columns= {'Frequency':'playlist_freq'})\n",
    "\n",
    "    # Add track frequency info, sort by playlist_freq first then by song_freq\n",
    "    new_one_1 = new_one.merge(most_freq_songs, left_on='track_uri', right_on='track_uri').sort_values(by=['playlist_freq', 'song_freq'], ascending=False)\n",
    "\n",
    "    # Exclude doublons from proposition\n",
    "    sans_doublons = new_one_1[['track_uri', 'playlist_freq', 'song_freq']].drop_duplicates(['track_uri'])\n",
    "\n",
    "    # First 500 tracks\n",
    "    recommended = sans_doublons[:n]['track_uri'].tolist()\n",
    "    \n",
    "    if len(recommended) < n:\n",
    "        songs_not_in_list = count_df[~count_df['track_uri'].isin(recommended)]['track_uri']\n",
    "        songs_to_add = songs_not_in_list.head(n - len(recommended)).tolist()\n",
    "        recommended.extend(songs_to_add)\n",
    "            \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommended = get_recommendation()\n",
    "#recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for a playlist\n",
    "def get_recommendation_for_list_of_tracks(playlist, n):\n",
    "\n",
    "    # if initial playlist is empty, return 500 most frequent tracks\n",
    "    if len(playlist) == 0:\n",
    "        return get_first_tracks(n)\n",
    "    \n",
    "    most_relevant = get_most_relevant(playlist)\n",
    "\n",
    "    pids = most_relevant['pid'].tolist()\n",
    "\n",
    "    # Exclude preferencies from proposition\n",
    "    sans_preferences = train_playlist_songs[train_playlist_songs.pid.isin(pids) & ~train_playlist_songs.track_uri.isin(playlist)][['pid', 'track_uri']]\n",
    "\n",
    "    # Add playlist frequency info\n",
    "    new_one = sans_preferences.merge(most_relevant, left_on='pid', right_on='pid').rename(columns= {'Frequency':'playlist_freq'})\n",
    "\n",
    "    # Add track frequency info, sort by playlist_freq first then by song_freq\n",
    "    new_one_1 = new_one.merge(most_freq_songs, left_on='track_uri', right_on='track_uri').sort_values(by=['playlist_freq', 'song_freq'], ascending=False)\n",
    "\n",
    "    # Exclude doublons from proposition\n",
    "    sans_doublons = new_one_1[['track_uri', 'playlist_freq', 'song_freq']].drop_duplicates(['track_uri'])\n",
    "\n",
    "    # First 500 tracks\n",
    "    recommended = sans_doublons[:n]['track_uri'].tolist()\n",
    "    \n",
    "    if len(recommended) < n:\n",
    "        songs_not_in_list = count_df[~count_df['track_uri'].isin(recommended)]['track_uri']\n",
    "        songs_to_add = songs_not_in_list.head(n - len(recommended)).tolist()\n",
    "        recommended.extend(songs_to_add)\n",
    "            \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using User-based recommendation with challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load challenge set\n",
    "\n",
    "path = \"data_challenge/challenge_set.json\"\n",
    "data = json.load(open(path,'r'))\n",
    "data_challenge = pd.DataFrame.from_dict(data['playlists'], orient='columns')\n",
    "\n",
    "challenge_set = [data_challenge]\n",
    "df_challenge = pd.concat(challenge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation for each playlist in challenge set, write to file\n",
    "\n",
    "import os\n",
    "\n",
    "file_name = 'result.csv'\n",
    "file_number = 1\n",
    "\n",
    "while os.path.exists(f\"{file_name[:-4]}_{file_number}.csv\"):\n",
    "    file_number += 1\n",
    "\n",
    "new_file_name = f\"{file_name[:-4]}_{file_number}.csv\"\n",
    "\n",
    "\n",
    "with open(new_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    if file.tell() == 0:\n",
    "        file.write('team_info,Bragina_Graff,vdfrtrp@gmail.com\\n\\n')\n",
    "\n",
    "    for i in range(1004, 1005):\n",
    "        playlist = df_challenge.iloc[i]\n",
    "\n",
    "        recommended = get_recommendation(playlist)\n",
    "        \n",
    "        pid = playlist['pid']\n",
    "        \n",
    "        recommended_str = ', '.join(recommended)\n",
    "        file.write(f\"{pid}, {recommended_str}\\n\\n\")\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(f\"playlist {i} was processed\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tracklist title analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('punkt')            #if arror need to download\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# A little cleaning\n",
    "def prepare_titles(df_train):\n",
    "    liste = (df_train[['name']]\n",
    "          .drop_duplicates(['name'])['name']\n",
    "          .str.lower()\n",
    "          .str.lstrip(string.punctuation)\n",
    "          .str.lstrip(string.whitespace)\n",
    "          .str.rstrip(string.punctuation)\n",
    "          .str.rstrip(string.whitespace)\n",
    "          .tolist()\n",
    "        )   \n",
    "    #remove doblicates again\n",
    "    return list(set(liste))\n",
    "\n",
    "\n",
    "\n",
    "# Find <k> the most similare titles of playlists for <title>\n",
    "def find_similar_titles(title, list_titles, k):    \n",
    "    simularity = {}\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    sample_embed = model.encode(title, convert_to_tensor=True)\n",
    "    \n",
    "    for el in list_titles:\n",
    "        \n",
    "        curr_embed = model.encode(el)\n",
    "    \n",
    "        #Compute cosine-similarities\n",
    "        cosine_scores = util.cos_sim(sample_embed, curr_embed)\n",
    "        \n",
    "        simularity[el] = cosine_scores.item()\n",
    "\n",
    "    # sort by values\n",
    "    ret = dict(sorted(simularity.items(), key=lambda x:x[1], reverse = True))\n",
    "\n",
    "    list_titles = list(ret.keys())\n",
    "    \n",
    "    return list_titles[:k]\n",
    "\n",
    "\n",
    "# Find synonimes for title in list_titles\n",
    "def find_sinonim_titles(title, list_titles):\n",
    "   \n",
    "    simularity = {}\n",
    "    \n",
    "    titles_text = \"\"\n",
    "    for p in list_titles: \n",
    "        titles_text += p\n",
    "    \n",
    "    titles_text = re.sub('[^a-z]', ' ', titles_text)\n",
    "    titles_text = re.sub(r'\\s+', ' ', titles_text)\n",
    "    \n",
    "\n",
    "    all_sentences = nltk.sent_tokenize(titles_text)\n",
    "    all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "    \n",
    "    \n",
    "    for i in range(len(all_words)):\n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]        # TODO need to add anither languages ?\n",
    "    \n",
    "    word2vec = Word2Vec(all_words, min_count=3)\n",
    "    return word2vec.wv.most_similar(title, topn=1)\n",
    "        \n",
    "    \n",
    "pref1 = ['spanish playlist']\n",
    "pref2 = ['uplift']\n",
    "pref3 = ['Groovin']\n",
    "\n",
    "list_titles = prepare_titles(train_playlist_songs)\n",
    "\n",
    "rez = find_similar_titles(pref2, list_titles, 10)\n",
    "rez\n",
    "\n",
    "syn = find_sinonim_titles(pref2, list_titles)\n",
    "#print(\"synonimes: \", syn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(algorithm, tracks):\n",
    "    # Divide each playlist into test and evaluation parts\n",
    "    tests = []\n",
    "    evaluations = []\n",
    "    for track in tracks:\n",
    "        tests.append(track[:int(len(track)*0.8)])\n",
    "        evaluations.append(track[int(len(track)*0.8):])\n",
    "\n",
    "    # Make recommendation for first 80% and compare to last 20%\n",
    "    recommended = []\n",
    "    for i in range(len(tests)):\n",
    "        recommended.append(algorithm(tests[i], 500))\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(len(recommended)):\n",
    "        intersection = set(recommended[i]) & set(evaluations[i])\n",
    "\n",
    "        precision = len(intersection) / len(recommended[i])\n",
    "        recall = len(intersection) / len(evaluations[i])\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Calculate average precision and recall for all playlists\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "\n",
    "    print(f\"Average precision: {avg_precision}\")\n",
    "    print(f\"Average recall: {avg_recall}\")\n",
    "\n",
    "    # Calculate F1 score\n",
    "    # Return average F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of list of tracks of each playlist\n",
    "\n",
    "playlist_tracks = evaluate_playlists_songs.groupby('pid')['track_uri'].apply(list).reset_index(name='tracks')\n",
    "\n",
    "playlists = playlist_tracks['tracks'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Average precision: 0.007\n",
      "Average recall: 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "tracks = playlists[:2]\n",
    "print(len(tracks))\n",
    "\n",
    "algorithm = lambda playlist, n: get_recommendation_for_list_of_tracks(playlist, n)\n",
    "evaluate_algorithm(algorithm, tracks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
